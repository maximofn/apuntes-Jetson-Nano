{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentación semántica con Segmentnet"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente capacidad de aprendizaje profundo que cubriremos en estos apuntes es la segmentación semántica. La segmentación semántica se basa en el reconocimiento de imágenes, excepto que las clasificaciones ocurren a nivel de píxel en oposición a la imagen completa. Esto se logra convolucionalizando una backbone de reconocimiento de imágenes previamente entrenada, que transforma el modelo en una Red totalmente convolucional (FCN) capaz de etiquetado por píxel. Especialmente útil para la percepción ambiental, la segmentación produce clasificaciones densas por píxel de muchos objetos potenciales diferentes por escena, incluidos los primeros planos y fondos de las escenas."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "`segNet` acepta como entrada la imagen 2D y genera una segunda imagen con la superposición de máscara de clasificación por píxel. Cada píxel de la máscara corresponde a la clase de objeto que se clasificó."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "La lista de objetos que se pueden detectar mediante las redes preentrenadas se encuentra en este [enlace](https://github.com/dusty-nv/jetson-inference/blob/master/data/networks/ssd_coco_labels.txt)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso de los programas precompilados de la Jetson"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí se pueden encontrar los códigos de los programas precompilados:\r\n",
        " * [C++](https://github.com/dusty-nv/jetson-inference/blob/master/examples/segnet/segnet.cpp)\r\n",
        " * [Python](https://github.com/dusty-nv/jetson-inference/blob/master/python/examples/segnet.py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de segmentación pre-entrenados disponibles"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se verán varios modelos de segmentación previamente entrenados disponibles que utilizan la red FCN-ResNet18 con rendimiento en tiempo real en Jetson. Se proporcionan modelos para una variedad de entornos y temas, incluidas ciudades urbanas, senderos todo terreno y espacios de oficinas y hogares en interiores.\r\n",
        "\r\n",
        "A continuación se muestra una tabla de los modelos de segmentación semántica previamente entrenados disponibles y el argumento --network asociado que se segnetutilizará para cargarlos. Están basados ​​en la red FCN-ResNet18 de clase 21 y han sido entrenados en varios conjuntos de datos y resoluciones usando [PyTorch](https://github.com/dusty-nv/pytorch-segmentation), y fueron exportados al [formato ONNX](https://onnx.ai/) para cargarlos con TensorRT.\r\n",
        "\r\n",
        "|Dataset|Resolution|Resolution|Accuracy (%)|Jetson Nano (FPS)|Jetson Xavier (FPS)|\r\n",
        "|-------|----------|----------|--------|-----------|-------------|\r\n",
        "|[Cityscapes](https://www.cityscapes-dataset.com/)|512x256|``fcn-resnet18-cityscapes-512x256``|83.3|48|480|\r\n",
        "|[Cityscapes](https://www.cityscapes-dataset.com/)|1024x512|``fcn-resnet18-cityscapes-1024x512``|87.3|12|175|\r\n",
        "|[Cityscapes](https://www.cityscapes-dataset.com/)|2048x1024|``fcn-resnet18-cityscapes-2048x1024``|89.6|3|47|\r\n",
        "|[DeepScene](http://deepscene.cs.uni-freiburg.de/)|576x320|``fcn-resnet18-deepscene-576x320``|96.4|26|360|\r\n",
        "|[DeepScene](http://deepscene.cs.uni-freiburg.de/)|864x480|``fcn-resnet18-deepscene-864x480``|96.9|14|190|\r\n",
        "|[Multi-Human](https://lv-mhp.github.io/)|512x320|``fcn-resnet18-mhp-512x320``|86.5|34|370|\r\n",
        "|[Multi-Human](https://lv-mhp.github.io/)|640x360|``fcn-resnet18-mhp-640x360``|87.1|23|325|\r\n",
        "|[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)|320x320|``fcn-resnet18-voc-320x320``|85.9|45|508|\r\n",
        "|[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)|512x320|``fcn-resnet18-voc-512x320``|88.5|34|375|\r\n",
        "|[SUN RGB-D](http://rgbd.cs.princeton.edu/)|512x400|``fcn-resnet18-sun-512x400``|64.3|28|340|\r\n",
        "|[SUN RGB-D](http://rgbd.cs.princeton.edu/)|640x512|``fcn-resnet18-sun-640x512``|65.1|17|224|\r\n",
        "\r\n",
        " * Si se omite la resolución del argumento CLI, se carga el modelo de resolución más baja\r\n",
        " * La precisión indica la precisión de la clasificación de píxeles en el conjunto de datos de validación del modelo.\r\n",
        " * El rendimiento se mide para el modo GPU FP16 con JetPack 4.2.1, `nvpmodel 0` (MAX-N)\r\n",
        "\r\n",
        " > **nota**: para descargar redes adicionales, ejecute la herramienta Model Downloader\r\n",
        " \r\n",
        " >           ``$ cd jetson-inference/tools``\r\n",
        " >           ``$ ./download-models.sh``"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso de los programas precompilados de la Jetson"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además de las rutas de entrada/salida, hay algunas opciones de línea de comandos adicionales:\r\n",
        "\r\n",
        " * flag ``--network`` (opcional) cambia el modelo de segmentación que se está utilizando (ver arriba)\r\n",
        " * flag ``--visualize`` (opcional) acepta ``mask`` y/o ``overlay`` (el valor predeterminado es ``overlay,mask``)\r\n",
        " * flag ``--alpha`` (opcional) establece el valor de fusión alfa para ``overlay`` (el valor predeterminado es 120)\r\n",
        " * flag ``--filter-mode`` (opcional) acepta ``point`` o ``linear`` (el valor predeterminado es ``linear``)\r\n",
        "\r\n",
        "Usar el flag ``--help`` para obtener más información"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ir a la carpeta con los programas precompilados con el siguiente comando\r\n",
        "\r\n",
        "```\r\n",
        "$ cd jetson-inference/build/aarch64/bin\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comandos:\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ $ ./segnet --network=<model> input.jpg output.jpg\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=<model> input.jpg output.jpg\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cityscapes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se muestra un ejemplo de segmentación de una escena de una calle urbana con el modelo [Cityscapes](https://www.cityscapes-dataset.com/).\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-cityscapes images/city_0.jpg images/test/city_0.jpg\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-cityscapes images/city_0.jpg images/test/city_0.jpg\r\n",
        "```\r\n",
        "\r\n",
        "![Cityscapes](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-city.jpg)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepScene"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "El conjunto de datos [DeepScene](http://deepscene.cs.uni-freiburg.de/) consta de senderos forestales y vegetación fuera de la carretera, lo que ayuda a los robots al aire libre en el seguimiento de caminos.\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-deepscene images/trail_0.jpg images/test/trail_0.jpg\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-deepscene images/trail_0.jpg images/test/trail_0.jpg\r\n",
        "```\r\n",
        "\r\n",
        "![DeepScene](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-deepscene-0-overlay.jpg)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Human Parsing (MHP)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Multi-Human Parsing](https://lv-mhp.github.io/) proporciona un etiquetado denso de las partes del cuerpo, como brazos, piernas, cabeza y diferentes tipos de ropa.\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-mhp images/humans_0.jpg images/test/humans_0.jpg\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-mhp images/humans_0.jpg images/test/humans_0.jpg\r\n",
        "```\r\n",
        "\r\n",
        "![segmentation-mhp-0](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-mhp-0.jpg)\r\n",
        "\r\n",
        "![segmentation-mhp-1](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-mhp-1.jpg)\r\n",
        "\r\n",
        "![segmentation-mhp-legend](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-mhp-legend.jpg)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pascal VOC"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) es uno de los conjuntos de datos originales utilizados para la segmentación semántica, que contiene varias personas, animales, vehículos y objetos domésticos.\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-voc images/object_0.jpg images/test/object_0.jpg\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-voc images/object_0.jpg images/test/object_0.jpg\r\n",
        "```\r\n",
        "\r\n",
        "![segmentation-voc](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-voc.jpg)\r\n",
        "\r\n",
        "![segmentation-voc-legend](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-voc-legend.jpg)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUN RGB-D"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "El conjunto de datos [SUN RGB-D](http://rgbd.cs.princeton.edu/) proporciona segmentación del terreno para muchos objetos y escenas interiores que se encuentran comúnmente en espacios de oficinas y hogares.\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-sun images/room_0.jpg images/test/room_0.jpg\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-sun images/room_0.jpg images/test/room_0.jpg\r\n",
        "```\r\n",
        "\r\n",
        "![segmentation-sun](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-sun.jpg)\r\n",
        "\r\n",
        "![segmentation-sun-legend](https://raw.githubusercontent.com/dusty-nv/jetson-inference/pytorch/docs/images/segmentation-sun-legend.jpg)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procesamiento de varias imágenes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos detectar varias imágenes\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-sun \"images/room_*.jpg\" \"images/test/room_%i.jpg\"\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-sun \"images/room_*.jpg\" \"images/test/room_%i.jpg\"\r\n",
        "```\r\n",
        "\r\n",
        " > **nota**: cuando se usen asteriscos, hay que escribirlos siempre entre comillas (\"*.jpg\"). De lo contrario, el sistema operativo expandirá automáticamente la secuencia y modificará el orden de los argumentos en la línea de comandos, lo que puede resultar en que una de las imágenes de entrada sea sobrescrita por la salida."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos cambiar eñ tipo de visualización hay que usar el flag `--visualize`. Podemos usar `mask` y `overlay` (por defecto `overlay,mask`)\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-mhp --visualize=mask images/humans_0.jpg images/test/humans_0.jpg         # Only mask\r\n",
        "$ ./segnet --network=fcn-resnet18-mhp --visualize=overlay images/humans_1.jpg images/test/humans_1.jpg      # Only overlay\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-mhp --visualize=mask images/humans_0.jpg images/test/humans_0.jpg      # Only mask\r\n",
        "$ ./segnet.py --network=fcn-resnet18-mhp --visualize=overlay images/humans_1.jpg images/test/humans_1.jpg   # Only overlay\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transparencia"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si el modo de visualización es `overlay`, se puede elegir la transparencia de la máscara mediante el flag `--alpha`, cuanto más pequeño es el valor, más transparente es la máscara\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-mhp --visualize=overlay --alpha=50 images/humans_0.jpg images/test/humans_0.jpg      # Alpha 50\r\n",
        "$ ./segnet --network=fcn-resnet18-mhp --visualize=overlay --alpha=200 images/humans_1.jpg images/test/humans_1.jpg     # Alpha 200\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-mhp --visualize=overlay --alpha=50 images/humans_0.jpg images/test/humans_0.jpg      # Alpha 50\r\n",
        "$ ./segnet.py --network=fcn-resnet18-mhp --visualize=overlay --alpha=200 images/humans_1.jpg images/test/humans_1.jpg     # Alpha 200\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tipo de filtro"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede elegir el tipo de filtro mediante el flag `--filter-mode`. Podemos usar `linear` o `point` (por defecto `linear`)\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-mhp --filter-mode=linear images/humans_0.jpg images/test/humans_0.jpg    # Filter mode linear\r\n",
        "$ ./segnet --network=fcn-resnet18-mhp --filter-mode=point images/humans_1.jpg images/test/humans_1.jpg     # Filter mode point\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-mhp --filter-mode=linear images/humans_0.jpg images/test/humans_0.jpg    # Filter mode linear\r\n",
        "$ ./segnet.py --network=fcn-resnet18-mhp --filter-mode=point images/humans_1.jpg images/test/humans_1.jpg     # Filter mode point\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmentación de vídeos"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si se quiere procesar un videdo solo hay que indicarlo en la entrada\r\n",
        "\r\n",
        "Para ello ejecutamos el docker montando la carpeta del SDK de VisionWorks\r\n",
        "\r\n",
        "```\r\n",
        "$ docker/run.sh --volume /usr/share/visionworks/sources/data:/videos\r\n",
        "```\r\n",
        "\r\n",
        "Y ya lo podemos procesar\r\n",
        "\r\n",
        "```\r\n",
        "# C++\r\n",
        "$ ./segnet --network=fcn-resnet18-cityscapes /videos/cars.mp4 images/test/cars_segmentation.mp4\r\n",
        "\r\n",
        "# Python\r\n",
        "$ ./segnet.py --network=fcn-resnet18-cityscapes /videos/cars.mp4 images/test/cars_segmentation.mp4\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear un programa de clasificación en Python"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vamos a crear un programa, lo primero que tenemos que hacer es crear una carpeta en el Host donde guardaremos el programa\r\n",
        "\r\n",
        "```\r\n",
        "$ cd ~/\r\n",
        "$ mkdir my-segmentation-python\r\n",
        "$ cd my-segmentation-python\r\n",
        "$ touch my-segmentation.py\r\n",
        "$ chmod +x my-segmentation.py\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos descargamos unas imágenes de osos para probar\r\n",
        "\r\n",
        "```\r\n",
        "$ wget https://github.com/dusty-nv/jetson-inference/raw/master/data/images/black_bear.jpg\r\n",
        "$ wget https://github.com/dusty-nv/jetson-inference/raw/master/data/images/brown_bear.jpg\r\n",
        "$ wget https://github.com/dusty-nv/jetson-inference/raw/master/data/images/polar_bear.jpg\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación lo que hay que hacer es lanzar el Docker con una carpeta del Host compartida, para que así cuando se cierre el Docker no se borre el programa, para ello lanzamos el Docker con el siguiente comando\r\n",
        "\r\n",
        "```\r\n",
        "$ docker/run.sh --volume ~/my-segmentation-python:/my-segmentation-python   # mounted inside the container to /my-segmentation-python\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez dentro del Docker ir a la carpeta con los siguientes comandos\r\n",
        "\r\n",
        "```\r\n",
        "$ cd ../\r\n",
        "$ cd my-segmentation-python\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Editar el archivo .py con el siguiente comando\r\n",
        "\r\n",
        "```\r\n",
        "$ nano my-segmentation.py\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para crear un programa como el precompilado escribimos el siguiente código\r\n",
        "\r\n",
        "```Python\r\n",
        "import jetson.inference\r\n",
        "import jetson.utils\r\n",
        "\r\n",
        "import argparse\r\n",
        "import sys\r\n",
        "\r\n",
        "# Add /jetson-inference/python/examples path for import segnet_utils\r\n",
        "sys.path.append('/jetson-inference/python/examples')\r\n",
        "from segnet_utils import *\r\n",
        "\r\n",
        "# parse the command line\r\n",
        "parser = argparse.ArgumentParser(description=\"Segment a live camera stream using an semantic segmentation DNN.\", \r\n",
        "                                 formatter_class=argparse.RawTextHelpFormatter, epilog=jetson.inference.segNet.Usage() +\r\n",
        "                                 jetson.utils.videoSource.Usage() + jetson.utils.videoOutput.Usage() + jetson.utils.logUsage())\r\n",
        "\r\n",
        "parser.add_argument(\"input_URI\", type=str, default=\"\", nargs='?', help=\"URI of the input stream\")\r\n",
        "parser.add_argument(\"output_URI\", type=str, default=\"\", nargs='?', help=\"URI of the output stream\")\r\n",
        "parser.add_argument(\"--network\", type=str, default=\"fcn-resnet18-voc\", help=\"pre-trained model to load, see below for options\")\r\n",
        "parser.add_argument(\"--filter-mode\", type=str, default=\"linear\", choices=[\"point\", \"linear\"], help=\"filtering mode used during visualization, options are:\\n  'point' or 'linear' (default: 'linear')\")\r\n",
        "parser.add_argument(\"--visualize\", type=str, default=\"overlay,mask\", help=\"Visualization options (can be 'overlay' 'mask' 'overlay,mask'\")\r\n",
        "parser.add_argument(\"--ignore-class\", type=str, default=\"void\", help=\"optional name of class to ignore in the visualization results (default: 'void')\")\r\n",
        "parser.add_argument(\"--alpha\", type=float, default=150.0, help=\"alpha blending value to use during overlay, between 0.0 and 255.0 (default: 150.0)\")\r\n",
        "parser.add_argument(\"--stats\", action=\"store_true\", help=\"compute statistics about segmentation mask class output\")\r\n",
        "\r\n",
        "is_headless = [\"--headless\"] if sys.argv[0].find('console.py') != -1 else [\"\"]\r\n",
        "\r\n",
        "try:\r\n",
        "\topt = parser.parse_known_args()[0]\r\n",
        "except:\r\n",
        "\tprint(\"\")\r\n",
        "\tparser.print_help()\r\n",
        "\tsys.exit(0)\r\n",
        "\r\n",
        "# load the segmentation network\r\n",
        "net = jetson.inference.segNet(opt.network, sys.argv)\r\n",
        "\r\n",
        "# set the alpha blending value\r\n",
        "net.SetOverlayAlpha(opt.alpha)\r\n",
        "\r\n",
        "# create video output\r\n",
        "output = jetson.utils.videoOutput(opt.output_URI, argv=sys.argv+is_headless)\r\n",
        "\r\n",
        "# create buffer manager\r\n",
        "buffers = segmentationBuffers(net, opt)\r\n",
        "\r\n",
        "# create video source\r\n",
        "input = jetson.utils.videoSource(opt.input_URI, argv=sys.argv)\r\n",
        "\r\n",
        "# process frames until user exits\r\n",
        "while True:\r\n",
        "\t# capture the next image\r\n",
        "\timg_input = input.Capture()\r\n",
        "\r\n",
        "\t# allocate buffers for this size image\r\n",
        "\tbuffers.Alloc(img_input.shape, img_input.format)\r\n",
        "\r\n",
        "\t# process the segmentation network\r\n",
        "\tnet.Process(img_input, ignore_class=opt.ignore_class)\r\n",
        "\r\n",
        "\t# generate the overlay\r\n",
        "\tif buffers.overlay:\r\n",
        "\t\tnet.Overlay(buffers.overlay, filter_mode=opt.filter_mode)\r\n",
        "\r\n",
        "\t# generate the mask\r\n",
        "\tif buffers.mask:\r\n",
        "\t\tnet.Mask(buffers.mask, filter_mode=opt.filter_mode)\r\n",
        "\r\n",
        "\t# composite the images\r\n",
        "\tif buffers.composite:\r\n",
        "\t\tjetson.utils.cudaOverlay(buffers.overlay, buffers.composite, 0, 0)\r\n",
        "\t\tjetson.utils.cudaOverlay(buffers.mask, buffers.composite, buffers.overlay.width, 0)\r\n",
        "\r\n",
        "\t# render the output image\r\n",
        "\toutput.Render(buffers.output)\r\n",
        "\r\n",
        "\t# update the title bar\r\n",
        "\toutput.SetStatus(\"{:s} | Network {:.0f} FPS\".format(opt.network, net.GetNetworkFPS()))\r\n",
        "\r\n",
        "\t# print out performance info\r\n",
        "\tjetson.utils.cudaDeviceSynchronize()\r\n",
        "\tnet.PrintProfilerTimes()\r\n",
        "\r\n",
        "    # compute segmentation class stats\r\n",
        "\tif opt.stats:\r\n",
        "\t\tbuffers.ComputeStats()\r\n",
        "    \r\n",
        "\t# exit on input/output EOS\r\n",
        "\tif not input.IsStreaming() or not output.IsStreaming():\r\n",
        "\t\tbreak\r\n",
        "\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutar el programa con el siguiente comando\r\n",
        "\r\n",
        "```\r\n",
        "$ python3 my-segmentation.py /dev/video0\r\n",
        "```\r\n",
        "\r\n",
        "En este caso abrirá la webcam, se pueden introducir las mismas variables que con el programa precompilado"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear un programa ded clasificación en C++"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vamos a crear un programa, lo primero que tenemos que hacer es crear una carpeta en el Host donde guardaremos el programa\r\n",
        "\r\n",
        "```\r\n",
        "$ cd ~/\r\n",
        "$ mkdir my-segmentation-cpp\r\n",
        "$ cd my-segmentation-cpp\r\n",
        "$ touch my-segmentation.cpp\r\n",
        "$ chmod +x my-segmentation.cpp\r\n",
        "$ touch CMakeLists.txt\r\n",
        "$ chmod +x CMakeLists.txt\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos descargamos unas imágenes de osos para probar\r\n",
        "\r\n",
        "```\r\n",
        "$ wget https://github.com/dusty-nv/jetson-inference/raw/master/data/images/black_bear.jpg\r\n",
        "$ wget https://github.com/dusty-nv/jetson-inference/raw/master/data/images/brown_bear.jpg\r\n",
        "$ wget https://github.com/dusty-nv/jetson-inference/raw/master/data/images/polar_bear.jpg\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación lo que hay que hacer es lanzar el Docker con una carpeta del Host compartida, para que así cuando se cierre el Docker no se borre el programa, para ello lanzamos el Docker con el siguiente comando\r\n",
        "\r\n",
        "```\r\n",
        "$ docker/run.sh --volume ~/my-segmentation-cpp:/my-segmentation-cpp   # mounted inside the container to /my-segmentation-cpp\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez dentro del Docker ir a la carpeta con los siguientes comandos\r\n",
        "\r\n",
        "```\r\n",
        "$ cd ../\r\n",
        "$ cd my-segmentation-cpp\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Editar el archivo .py con el siguiente comando\r\n",
        "\r\n",
        "```\r\n",
        "$ nano my-segmentation.cpp\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para crear un programa como el precompilado escribimos el siguiente código\r\n",
        "\r\n",
        "```C++\r\n",
        "#include <jetson-utils/videoSource.h>\r\n",
        "#include <jetson-utils/videoOutput.h>\r\n",
        "\r\n",
        "#include <jetson-utils/cudaFont.h>\r\n",
        "\r\n",
        "#include <jetson-utils/cudaOverlay.h>\r\n",
        "#include <jetson-utils/cudaMappedMemory.h>\r\n",
        "\r\n",
        "#include <jetson-inference/segNet.h>\r\n",
        "\r\n",
        "#include <signal.h>\r\n",
        "\r\n",
        "\r\n",
        "#ifdef HEADLESS\r\n",
        "\t#define IS_HEADLESS() \"headless\"             // run without display\r\n",
        "\t#define DEFAULT_VISUALIZATION \"overlay\"      // output overlay only\r\n",
        "#else\r\n",
        "\t#define IS_HEADLESS() (const char*)NULL      // use display (if attached)\r\n",
        "\t#define DEFAULT_VISUALIZATION \"overlay|mask\" // output overlay + mask\r\n",
        "#endif\r\n",
        "\r\n",
        "\r\n",
        "bool signal_recieved = false;\r\n",
        "\r\n",
        "void sig_handler(int signo)\r\n",
        "{\r\n",
        "\tif( signo == SIGINT )\r\n",
        "\t{\r\n",
        "\t\tLogVerbose(\"received SIGINT\\n\");\r\n",
        "\t\tsignal_recieved = true;\r\n",
        "\t}\r\n",
        "}\r\n",
        "\r\n",
        "int usage()\r\n",
        "{\r\n",
        "\tprintf(\"usage: segnet [--help] [--network NETWORK] ...\\n\");\r\n",
        "\tprintf(\"              input_URI [output_URI]\\n\\n\");\r\n",
        "\tprintf(\"Segment and classify a video/image stream using a semantic segmentation DNN.\\n\");\r\n",
        "\tprintf(\"See below for additional arguments that may not be shown above.\\n\\n\");\r\n",
        "\tprintf(\"positional arguments:\\n\");\r\n",
        "\tprintf(\"    input_URI       resource URI of input stream  (see videoSource below)\\n\");\r\n",
        "\tprintf(\"    output_URI      resource URI of output stream (see videoOutput below)\\n\\n\");\r\n",
        "\r\n",
        "\tprintf(\"%s\\n\", segNet::Usage());\r\n",
        "\tprintf(\"%s\\n\", videoSource::Usage());\r\n",
        "\tprintf(\"%s\\n\", videoOutput::Usage());\r\n",
        "\tprintf(\"%s\\n\", Log::Usage());\r\n",
        "\r\n",
        "\treturn 0;\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "//\r\n",
        "// segmentation buffers\r\n",
        "//\r\n",
        "typedef uchar3 pixelType;\t\t// this can be uchar3, uchar4, float3, float4\r\n",
        "\r\n",
        "pixelType* imgMask      = NULL;\t// color of each segmentation class\r\n",
        "pixelType* imgOverlay   = NULL;\t// input + alpha-blended mask\r\n",
        "pixelType* imgComposite = NULL;\t// overlay with mask next to it\r\n",
        "pixelType* imgOutput    = NULL;\t// reference to one of the above three\r\n",
        "\r\n",
        "int2 maskSize;\r\n",
        "int2 overlaySize;\r\n",
        "int2 compositeSize;\r\n",
        "int2 outputSize;\r\n",
        "\r\n",
        "// allocate mask/overlay output buffers\r\n",
        "bool allocBuffers( int width, int height, uint32_t flags )\r\n",
        "{\r\n",
        "\t// check if the buffers were already allocated for this size\r\n",
        "\tif( imgOverlay != NULL && width == overlaySize.x && height == overlaySize.y )\r\n",
        "\t\treturn true;\r\n",
        "\r\n",
        "\t// free previous buffers if they exit\r\n",
        "\tCUDA_FREE_HOST(imgMask);\r\n",
        "\tCUDA_FREE_HOST(imgOverlay);\r\n",
        "\tCUDA_FREE_HOST(imgComposite);\r\n",
        "\r\n",
        "\t// allocate overlay image\r\n",
        "\toverlaySize = make_int2(width, height);\r\n",
        "\t\r\n",
        "\tif( flags & segNet::VISUALIZE_OVERLAY )\r\n",
        "\t{\r\n",
        "\t\tif( !cudaAllocMapped(&imgOverlay, overlaySize) )\r\n",
        "\t\t{\r\n",
        "\t\t\tLogError(\"segnet:  failed to allocate CUDA memory for overlay image (%ux%u)\\n\", width, height);\r\n",
        "\t\t\treturn false;\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\timgOutput = imgOverlay;\r\n",
        "\t\toutputSize = overlaySize;\r\n",
        "\t}\r\n",
        "\r\n",
        "\t// allocate mask image (half the size, unless it's the only output)\r\n",
        "\tif( flags & segNet::VISUALIZE_MASK )\r\n",
        "\t{\r\n",
        "\t\tmaskSize = (flags & segNet::VISUALIZE_OVERLAY) ? make_int2(width/2, height/2) : overlaySize;\r\n",
        "\r\n",
        "\t\tif( !cudaAllocMapped(&imgMask, maskSize) )\r\n",
        "\t\t{\r\n",
        "\t\t\tLogError(\"segnet:  failed to allocate CUDA memory for mask image\\n\");\r\n",
        "\t\t\treturn false;\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\timgOutput = imgMask;\r\n",
        "\t\toutputSize = maskSize;\r\n",
        "\t}\r\n",
        "\r\n",
        "\t// allocate composite image if both overlay and mask are used\r\n",
        "\tif( (flags & segNet::VISUALIZE_OVERLAY) && (flags & segNet::VISUALIZE_MASK) )\r\n",
        "\t{\r\n",
        "\t\tcompositeSize = make_int2(overlaySize.x + maskSize.x, overlaySize.y);\r\n",
        "\r\n",
        "\t\tif( !cudaAllocMapped(&imgComposite, compositeSize) )\r\n",
        "\t\t{\r\n",
        "\t\t\tLogError(\"segnet:  failed to allocate CUDA memory for composite image\\n\");\r\n",
        "\t\t\treturn false;\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\timgOutput = imgComposite;\r\n",
        "\t\toutputSize = compositeSize;\r\n",
        "\t}\r\n",
        "\r\n",
        "\treturn true;\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "int main( int argc, char** argv )\r\n",
        "{\r\n",
        "\t/*\r\n",
        "\t * parse command line\r\n",
        "\t */\r\n",
        "\tcommandLine cmdLine(argc, argv, IS_HEADLESS());\r\n",
        "\r\n",
        "\tif( cmdLine.GetFlag(\"help\") )\r\n",
        "\t\treturn usage();\r\n",
        "\r\n",
        "\r\n",
        "\t/*\r\n",
        "\t * attach signal handler\r\n",
        "\t */\r\n",
        "\tif( signal(SIGINT, sig_handler) == SIG_ERR )\r\n",
        "\t\tLogError(\"can't catch SIGINT\\n\");\r\n",
        "\r\n",
        "\r\n",
        "\t/*\r\n",
        "\t * create input stream\r\n",
        "\t */\r\n",
        "\tvideoSource* input = videoSource::Create(cmdLine, ARG_POSITION(0));\r\n",
        "\r\n",
        "\tif( !input )\r\n",
        "\t{\r\n",
        "\t\tLogError(\"segnet:  failed to create input stream\\n\");\r\n",
        "\t\treturn 0;\r\n",
        "\t}\r\n",
        "\r\n",
        "\r\n",
        "\t/*\r\n",
        "\t * create output stream\r\n",
        "\t */\r\n",
        "\tvideoOutput* output = videoOutput::Create(cmdLine, ARG_POSITION(1));\r\n",
        "\t\r\n",
        "\tif( !output )\r\n",
        "\t\tLogError(\"segnet:  failed to create output stream\\n\");\t\r\n",
        "\t\r\n",
        "\r\n",
        "\t/*\r\n",
        "\t * create segmentation network\r\n",
        "\t */\r\n",
        "\tsegNet* net = segNet::Create(cmdLine);\r\n",
        "\t\r\n",
        "\tif( !net )\r\n",
        "\t{\r\n",
        "\t\tLogError(\"segnet:  failed to initialize segNet\\n\");\r\n",
        "\t\treturn 0;\r\n",
        "\t}\r\n",
        "\r\n",
        "\t// set alpha blending value for classes that don't explicitly already have an alpha\t\r\n",
        "\tnet->SetOverlayAlpha(cmdLine.GetFloat(\"alpha\", 150.0f));\r\n",
        "\r\n",
        "\t// get the desired overlay/mask filtering mode\r\n",
        "\tconst segNet::FilterMode filterMode = segNet::FilterModeFromStr(cmdLine.GetString(\"filter-mode\", \"linear\"));\r\n",
        "\r\n",
        "\t// get the visualization flags\r\n",
        "\tconst uint32_t visualizationFlags = segNet::VisualizationFlagsFromStr(cmdLine.GetString(\"visualize\", DEFAULT_VISUALIZATION));\r\n",
        "\r\n",
        "\t// get the object class to ignore (if any)\r\n",
        "\tconst char* ignoreClass = cmdLine.GetString(\"ignore-class\", \"void\");\r\n",
        "\r\n",
        "\t\r\n",
        "\t\r\n",
        "\t/*\r\n",
        "\t * processing loop\r\n",
        "\t */\r\n",
        "\twhile( !signal_recieved )\r\n",
        "\t{\r\n",
        "\t\t// capture next image image\r\n",
        "\t\tpixelType* imgInput = NULL;\r\n",
        "\r\n",
        "\t\tif( !input->Capture(&imgInput, 1000) )\r\n",
        "\t\t{\r\n",
        "\t\t\t// check for EOS\r\n",
        "\t\t\tif( !input->IsStreaming() )\r\n",
        "\t\t\t\tbreak; \r\n",
        "\r\n",
        "\t\t\tLogError(\"segnet:  failed to capture video frame\\n\");\r\n",
        "\t\t\tcontinue;\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\t// allocate buffers for this size frame\r\n",
        "\t\tif( !allocBuffers(input->GetWidth(), input->GetHeight(), visualizationFlags) )\r\n",
        "\t\t{\r\n",
        "\t\t\tLogError(\"segnet:  failed to allocate buffers\\n\");\r\n",
        "\t\t\tcontinue;\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\t// process the segmentation network\r\n",
        "\t\tif( !net->Process(imgInput, input->GetWidth(), input->GetHeight(), ignoreClass) )\r\n",
        "\t\t{\r\n",
        "\t\t\tLogError(\"segnet:  failed to process segmentation\\n\");\r\n",
        "\t\t\tcontinue;\r\n",
        "\t\t}\r\n",
        "\t\t\r\n",
        "\t\t// generate overlay\r\n",
        "\t\tif( visualizationFlags & segNet::VISUALIZE_OVERLAY )\r\n",
        "\t\t{\r\n",
        "\t\t\tif( !net->Overlay(imgOverlay, overlaySize.x, overlaySize.y, filterMode) )\r\n",
        "\t\t\t{\r\n",
        "\t\t\t\tLogError(\"segnet:  failed to process segmentation overlay.\\n\");\r\n",
        "\t\t\t\tcontinue;\r\n",
        "\t\t\t}\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\t// generate mask\r\n",
        "\t\tif( visualizationFlags & segNet::VISUALIZE_MASK )\r\n",
        "\t\t{\r\n",
        "\t\t\tif( !net->Mask(imgMask, maskSize.x, maskSize.y, filterMode) )\r\n",
        "\t\t\t{\r\n",
        "\t\t\t\tLogError(\"segnet:-console:  failed to process segmentation mask.\\n\");\r\n",
        "\t\t\t\tcontinue;\r\n",
        "\t\t\t}\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\t// generate composite\r\n",
        "\t\tif( (visualizationFlags & segNet::VISUALIZE_OVERLAY) && (visualizationFlags & segNet::VISUALIZE_MASK) )\r\n",
        "\t\t{\r\n",
        "\t\t\tCUDA(cudaOverlay(imgOverlay, overlaySize, imgComposite, compositeSize, 0, 0));\r\n",
        "\t\t\tCUDA(cudaOverlay(imgMask, maskSize, imgComposite, compositeSize, overlaySize.x, 0));\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\t// render outputs\r\n",
        "\t\tif( output != NULL )\r\n",
        "\t\t{\r\n",
        "\t\t\toutput->Render(imgOutput, outputSize.x, outputSize.y);\r\n",
        "\r\n",
        "\t\t\t// update the status bar\r\n",
        "\t\t\tchar str[256];\r\n",
        "\t\t\tsprintf(str, \"TensorRT %i.%i.%i | %s | Network %.0f FPS\", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR, NV_TENSORRT_PATCH, net->GetNetworkName(), net->GetNetworkFPS());\r\n",
        "\t\t\toutput->SetStatus(str);\r\n",
        "\r\n",
        "\t\t\t// check if the user quit\r\n",
        "\t\t\tif( !output->IsStreaming() )\r\n",
        "\t\t\t\tsignal_recieved = true;\r\n",
        "\t\t}\r\n",
        "\r\n",
        "\t\t// wait for the GPU to finish\t\t\r\n",
        "\t\tCUDA(cudaDeviceSynchronize());\r\n",
        "\r\n",
        "\t\t// print out timing info\r\n",
        "\t\tnet->PrintProfilerTimes();\r\n",
        "\t}\r\n",
        "\t\r\n",
        "\r\n",
        "\t/*\r\n",
        "\t * destroy resources\r\n",
        "\t */\r\n",
        "\tLogVerbose(\"segnet:  shutting down...\\n\");\r\n",
        "\t\r\n",
        "\tSAFE_DELETE(input);\r\n",
        "\tSAFE_DELETE(output);\r\n",
        "\tSAFE_DELETE(net);\r\n",
        "\r\n",
        "\tCUDA_FREE_HOST(imgMask);\r\n",
        "\tCUDA_FREE_HOST(imgOverlay);\r\n",
        "\tCUDA_FREE_HOST(imgComposite);\r\n",
        "\r\n",
        "\tLogVerbose(\"segnet:  shutdown complete.\\n\");\r\n",
        "\treturn 0;\r\n",
        "}\r\n",
        "\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Editar el CMakeList.txt con lo siguiente\r\n",
        "\r\n",
        "```\r\n",
        "# require CMake 2.8 or greater\r\n",
        "cmake_minimum_required(VERSION 2.8)\r\n",
        "\r\n",
        "# declare my-segmentation project\r\n",
        "project(my-segmentation)\r\n",
        "\r\n",
        "# import jetson-inference and jetson-utils packages.\r\n",
        "# note that if you didn't do \"sudo make install\"\r\n",
        "# while building jetson-inference, this will error.\r\n",
        "find_package(jetson-utils)\r\n",
        "find_package(jetson-inference)\r\n",
        "\r\n",
        "# CUDA and Qt4 are required\r\n",
        "find_package(CUDA)\r\n",
        "find_package(Qt4)\r\n",
        "\r\n",
        "# setup Qt4 for build\r\n",
        "include(${QT_USE_FILE})\r\n",
        "add_definitions(${QT_DEFINITIONS})\r\n",
        "\r\n",
        "# compile the my-segmentation program\r\n",
        "cuda_add_executable(my-segmentation my-segmentation.cpp)\r\n",
        "\r\n",
        "# link my-segmentation to jetson-inference library\r\n",
        "target_link_libraries(my-segmentation jetson-inference)\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilar el código con los siguientes comandos\r\n",
        "\r\n",
        "```\r\n",
        "$ cmake .\r\n",
        "$ make\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutar el programa con el siguiente comando\r\n",
        "\r\n",
        "```\r\n",
        "$ ./my-segmentation /dev/video0\r\n",
        "```\r\n",
        "\r\n",
        "En este caso abrirá la webcam, se pueden introducir las mismas variables que con el programa precompilado"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.11 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}